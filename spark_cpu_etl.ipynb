{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093fc6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pyspark.sql.functions as fn\n",
    "import shutil\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024cd412-e5d5-4b0a-9e48-50424de38c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(start):\n",
    "    print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cdcd3-0104-4c6f-ba61-8a9e9a323ceb",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c3a2b4-b7c8-488d-bebf-4f6874760666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/01 09:44:21 WARN Utils: Your hostname, bdai-desktop resolves to a loopback address: 127.0.1.1; using 165.132.118.198 instead (on interface enp0s31f6)\n",
      "23/08/01 09:44:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/01 09:44:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.62 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCPU').config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f8569-f2e3-439f-898d-67a03513edd4",
   "metadata": {},
   "source": [
    "# 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97b1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagezip_path = \"/home/bdai/covid_data/covidx-cxr2.zip\"\n",
    "image_path = \"/home/bdai/spark_work/covid_dataset\"\n",
    "\n",
    "# shutil.unpack_archive(imagezip_path, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5562c2f1-49f4-49a1-9b74-2b66f3a75c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 13.07 GBs\n"
     ]
    }
   ],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "dir_size = round(get_dir_size(image_path) / (1024 ** 3),2)\n",
    "\n",
    "print(\"Total dataset size : {} GBs\".format(dir_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b986c265-068d-4b6a-8d12-8d6bf3918d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_images = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(image_path + \"/train\")\n",
    "test_images = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").load(image_path + \"/test\")\n",
    "\n",
    "# [patient id] [filename] [class] [data source] \n",
    "train_txt = spark.read.text(\"/home/bdai/spark_work/covid_dataset/train.txt\")\n",
    "test_txt = spark.read.text(\"/home/bdai/spark_work/covid_dataset/test.txt\")\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6316b-0247-472b-b33d-b13bea4e5bd2",
   "metadata": {},
   "source": [
    "# 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "819f8e5b-c897-4446-a122-18e9a4564e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_size (content):\n",
    "    # Extrach image size from its raw content\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    return image.size\n",
    "\n",
    "@fn.pandas_udf(\"width: int, height: int\")\n",
    "def extract_size_udf(content_series):\n",
    "    sizes = content_series.apply(extract_size)\n",
    "    return pd.DataFrame(list(sizes))\n",
    "\n",
    "\n",
    "def transform_merge(image, text):\n",
    "    image = image.withColumn(\"file_name\", fn.substring_index(image.path, \"/\", -1))\n",
    "    text = text.select(split(col(\"value\"),\" \").getItem(0).alias(\"patient_id\"),\n",
    "                       split(col(\"value\"),\" \").getItem(1).alias(\"file_name\"),\n",
    "                       split(col(\"value\"),\" \").getItem(2).alias(\"class\")).drop(\"value\")\n",
    "    df = image.join(text,['file_name'],how='inner')\n",
    "    df = df.select(fn.col(\"path\"),\n",
    "                   fn.col(\"file_name\"),\n",
    "                   extract_size_udf(fn.col(\"content\")).alias(\"size\"),\n",
    "                   fn.col(\"content\"),\n",
    "                   fn.col(\"class\"))\n",
    "    indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d511eac2-3f83-482c-938c-855cc59a928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 5.19 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df = transform_merge(train_images, train_txt)\n",
    "test_df = transform_merge(test_images, test_txt)\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b7b4f-1ab4-4a5e-90e7-90e318138ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = test_df.select(\"content\").collect()\n",
    "\n",
    "# from torchvision import transforms\n",
    "# for i in range(100):\n",
    "#     temp_image = Image.open(io.BytesIO(temp[i][\"content\"]))\n",
    "#     trans = transforms.ToTensor()\n",
    "#     print(trans(temp_image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3acc1d-2608-4335-874e-99765a1e7578",
   "metadata": {},
   "source": [
    "# 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe210365-ec3f-4ebb-9e7e-4cdfe373538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 213.07 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "compression = spark.conf.get(\"spark.sql.parquet.compression.codec\")\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"uncompressed\")\n",
    "\n",
    "train_df.write.format(\"parquet\").mode(\"overwrite\").option(\"mergeSchema\", True).saveAsTable(\"covid_train_binary\")\n",
    "test_df.write.format(\"parquet\").mode(\"overwrite\").option(\"mergeSchema\", True).saveAsTable(\"covid_test_binary\")\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", compression)\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5693ce-c6f0-451d-b75b-eaf713416ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
