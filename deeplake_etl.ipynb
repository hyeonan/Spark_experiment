{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f4ce7e-cfba-4472-b4c0-e6b89abbb8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bdai/anaconda3/envs/deep_lake/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.17) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dc0327-4132-492e-9f95-d467bcb50902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(start):\n",
    "    print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5653d3-9eb8-4636-8c8e-5468cc924abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bdai/deeplake_work/deeplake_test loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "image_path = \"/home/bdai/spark_work/covid_dataset\"\n",
    "save_path = \"/home/bdai/deeplake_work\"\n",
    "#train_data = deeplake.empty(save_path + \"/deeplake_train\")\n",
    "test_data = deeplake.empty(save_path + \"/deeplake_test\")\n",
    "# train_data = deeplake.load(save_path + \"/deeplake_train\")\n",
    "# test_data = deeplake.load(save_path + \"/deeplake_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88df632d-1125-4d3b-94c1-525d38f760ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(path, data=\"train\"):\n",
    "    images = []\n",
    "    for dir_path, dir_names, file_names in os.walk(path):\n",
    "        for file_name in file_names:\n",
    "            if os.path.splitext(file_name)[-1] == \".txt\" and os.path.splitext(file_name)[0] == data:\n",
    "                text = file_name\n",
    "            else:\n",
    "                path_end = os.path.split(dir_path)[-1]\n",
    "                if path_end == data:\n",
    "                    images.append(file_name)\n",
    "    return images, text            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a46a86f-e594-4510-a4cb-e7ade8952423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images, train_text = collect_files(image_path, \"train\")\n",
    "test_images, test_text = collect_files(image_path, \"test\")\n",
    "\n",
    "class_names = [\"positive\", \"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4e8bf11-930d-4ec8-9829-123c317cdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label x 실험\n",
    "\n",
    "# ds = train_data\n",
    "# path = image_path+\"/train\"\n",
    "# image_files = train_images\n",
    "def image_transformation(ds, path, image_files, text_file, class_names):\n",
    "# ds = train_data\n",
    "# path = image_path+\"/train\"\n",
    "# image_files = train_images\n",
    "# text_file = train_text\n",
    "    with ds:\n",
    "        # Create the tensors with names of your choice.\n",
    "        ds.create_tensor('images', htype = 'image', sample_compression = 'png')\n",
    "        ds.create_tensor('file_id', sample_compression = \"lz4\")\n",
    "        ds.create_tensor('labels', htype = 'class_label', class_names = class_names, sample_compression = \"lz4\")\n",
    "        \n",
    "        # Add arbitrary metadata - Optional\n",
    "        ds.info.update(description = 'Covid classification dataset')\n",
    "        info_data = pd.read_csv(image_path + \"/\" + text_file, sep = \" \", names=[\"id\", \"file_name\", \"label\", \"orgin\"], header=0)\n",
    "        for image in tqdm(image_files):\n",
    "            label_text = info_data.loc[info_data[\"file_name\"] == image, \"label\"]\n",
    "            if len(label_text) == 0 :\n",
    "                continue\n",
    "            label_num = class_names.index(label_text.values[0])\n",
    "            ds.append({'images' : deeplake.read(os.path.join(path, image)), 'file_id' : image, 'labels': np.uint32(label_num)})\n",
    "            #ds.append({'images' : deeplake.read(os.path.join(path, image)), 'file_id' : image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41994413-865c-4b5a-8d78-3e519cdbbefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 400/400 [00:13<00:00, 28.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 14.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "start = time.time()\n",
    "#image_transformation(train_data, image_path+\"/train\", train_images, train_text, class_names)\n",
    "image_transformation(test_data, image_path+\"/test\", test_images, test_text, class_names)\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99cd935f-2ce9-47aa-93a6-470d4684b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Paruet dataset size : 13.17 GBs\n",
      "Total Tensor storage dataset size : 13.0 GBs\n"
     ]
    }
   ],
   "source": [
    "spark_file = \"/home/bdai/spark_work/spark-warehouse\"\n",
    "deeplake_file = \"/home/bdai/deeplake_work\"\n",
    "\n",
    "def get_dir_size(path='.'):\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total\n",
    "spark_dir_size = round(get_dir_size(spark_file) / (1024 ** 3),2)\n",
    "deeplake_dir_size = round(get_dir_size(deeplake_file) / (1024 ** 3),2)\n",
    "\n",
    "print(\"Total Paruet dataset size : {} GBs\".format(spark_dir_size))\n",
    "print(\"Total Tensor storage dataset size : {} GBs\".format(deeplake_dir_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec757bdd-e93d-4a91-b950-e779f8ffc78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb9e57-e9af-48ee-a7f8-015d70a9a9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
