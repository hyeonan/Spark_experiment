{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f533b06-99f6-443a-bc91-7cb9733a9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import shutil\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad44e98c-b2f1-4995-804f-1c7e8e59e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(start):\n",
    "    print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc38141-ec20-4819-9f7b-ef2351bb7131",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcd3f2c-e8ee-4a74-baf8-6bc4c10b3f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/01 11:14:48 WARN Utils: Your hostname, bdai-desktop resolves to a loopback address: 127.0.1.1; using 165.132.118.198 instead (on interface enp0s31f6)\n",
      "23/08/01 11:14:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/01 11:14:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCPU').config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa6673-00b3-4ce6-80d1-4783fd5ba85a",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87460605-bc29-430d-b789-2d9d39c869b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = \"/home/bdai/spark_work/spark-warehouse/covid_train_binary\"\n",
    "test_image_path = \"/home/bdai/spark_work/spark-warehouse/covid_test_binary\"\n",
    "cache_path = \"file:///home/bdai/spark_work/petastorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e52230e-c6c5-4097-b238-92d8e2033ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df = spark.read.parquet(train_image_path)\n",
    "df_test = spark.read.parquet(test_image_path)\n",
    "\n",
    "df_train, df_val = train_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a00087-2e8e-4a7f-a9e9-a40a088b939d",
   "metadata": {},
   "source": [
    "# 2. Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed78f42-f3bd-43be-9660-d1a3d1eb3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from petastorm import TransformSpec\n",
    "\n",
    "image_shape = (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9fcc8-79ef-4c8b-a48e-991b3602331c",
   "metadata": {},
   "source": [
    "## 1) Cache the Spark DataFrame using Petastorm Spark converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f58c73-0a49-40fd-a37f-0795ca1061aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting floating-point columns to float32\n",
      "23/08/01 11:15:33 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 360/144\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 60 slabs, 114,587,997 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 114,587,997/114,587,997\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1611\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,611/1,731\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/128\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:3021\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 3,021/3,141\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:92\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/212\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:96\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/216\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/01 11:15:36 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 336/136\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 56 slabs, 108,896,667 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 108,896,667/108,896,667\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1379\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,379/1,491\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/120\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2695\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,695/2,807\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:60\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/172\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:60\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/172\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/01 11:15:37 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 360/144\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 60 slabs, 106,755,018 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 106,755,018/106,755,018\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1392\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,392/1,512\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/128\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2802\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,802/2,922\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/188\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:72\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/192\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/01 11:15:38 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 384/152\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 64 slabs, 112,952,667 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112,952,667/112,952,667\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1482\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,482/1,610\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/136\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2986\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,986/3,114\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/196\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:100\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/228\n",
      " }\n",
      "}\n",
      "\n",
      "Converting floating-point columns to float32                                    \n",
      "The median size 17372240 B (< 50 MB) of the parquet files is too small. Total size: 2608736962 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///home/bdai/spark_work/petastorm/20230801111728-appid-local-1690856089587-ad75f554-e7d1-4ba0-8820-754e94507027/part-00055-0e8a8c7c-0e74-428c-9b9f-898b2d24409c-c000.parquet, ...\n",
      "Converting floating-point columns to float32\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 23931, val: 6055, test : 400\n",
      "Elapsed time: 230.71 s\n"
     ]
    }
   ],
   "source": [
    "# Set a cache directory on DBFS FUSE for intermediate data.\n",
    "start = time.time()\n",
    "\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, cache_path)\n",
    "\n",
    "converter_train = make_spark_converter(df_train)\n",
    "converter_val = make_spark_converter(df_val)\n",
    "converter_test = make_spark_converter(df_test)\n",
    "\n",
    "print(f\"train: {len(converter_train)}, val: {len(converter_val)}, test : {len(converter_test)}\")\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09948a01-db59-46dd-93de-d14a5e22c932",
   "metadata": {},
   "source": [
    "## 2) Preprocess images\n",
    "Before feeding the dataset into the model, we need to decode the raw image bytes and apply standard ImageNet transforms. We recommend not doing this transformation on the Spark DataFrame since that will substantially increase the size of the intermediate files and might harm the performance. Instead, we recommend doing this transformation in a TransformSpec function in petastorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7118c5e-6552-4a6c-bb20-744f860d841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    image = Image.open(io.BytesIO(content)).resize([image_shape[1],image_shape[2]])\n",
    "    transformers = [transforms.Lambda(lambda image: image.convert('RGB'))]\n",
    "    transformers.extend([transforms.ToTensor()])\n",
    "    trans = transforms.Compose(transformers)\n",
    "    image_arr = trans(image)\n",
    "    return image_arr.numpy()\n",
    "    \n",
    "\n",
    "def transform_row(pd_batch):\n",
    "  \"\"\"\n",
    "  The input and output of this function must be pandas dataframes.\n",
    "  \"\"\"\n",
    "  pd_batch['features'] = pd_batch['content'].map(lambda x: preprocess(x))\n",
    "  pd_batch['label'] = pd_batch['label'].map(lambda x: int(x))\n",
    "  pd_batch = pd_batch.drop(labels=['content'], axis=1)\n",
    "  return pd_batch[['features', 'label']]\n",
    "\n",
    "def get_transform_spec():\n",
    "  # Note that the output shape of the `TransformSpec` is not automatically known by petastorm, \n",
    "  # so we need to specify the shape for new columns in `edit_fields` and specify the order of \n",
    "  # the output columns in `selected_fields`.\n",
    "  return TransformSpec(transform_row, \n",
    "                       edit_fields=[('features', np.float32, image_shape, False)], \n",
    "                       selected_fields=['features', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487492df-2a5b-4934-8c15-bfd720e3d2bc",
   "metadata": {},
   "source": [
    "## 3) Examining execution time for dataloading and transorming a batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0685da20-4eb8-49e6-be2c-1b94f478fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), batch_size=16) as train_dataloader:\n",
    "    train_dataloader_iter = iter(train_dataloader)\n",
    "    for idx, batch in enumerate(train_dataloader_iter):\n",
    "        if idx == 1: break\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b85923-a6ba-445a-aa32-1444e4bdeecb",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe5d367-85ec-4fa0-8fff-709042c6b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a10f4-d7e2-485d-b837-31d42cc0b06c",
   "metadata": {},
   "source": [
    "## 1) Get the model ResNet from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31319536-7a2f-443c-b956-a97e82a220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lr=0.001):\n",
    "  # Load a ResNet50 model from torchvision\n",
    "  model = torchvision.models.resnet50(pretrained=True)\n",
    "  # Freeze parameters in the feature extraction layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "  # Add a new classifier layer for transfer learning\n",
    "  num_ftrs = model.fc.in_features\n",
    "  # Parameters of newly constructed modules have requires_grad=True by default\n",
    "  model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3592c-c981-47d0-9b96-d1c1f0e9bb5e",
   "metadata": {},
   "source": [
    "## 2) Define the train and evaluate function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7416cea7-290b-4d47-8deb-bb79167bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, scheduler, \n",
    "                    train_dataloader_iter, steps_per_epoch, epoch, \n",
    "                    device):\n",
    "  model.train()  # Set model to training mode\n",
    "\n",
    "  # statistics\n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0\n",
    "\n",
    "  # Iterate over the data for one epoch.\n",
    "  for step in range(steps_per_epoch):\n",
    "    pd_batch = next(train_dataloader_iter)\n",
    "    inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device)\n",
    "    \n",
    "    # Track history in training\n",
    "    with torch.set_grad_enabled(True):\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # backward + optimize\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "  \n",
    "  scheduler.step()\n",
    "\n",
    "  epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n",
    "  epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n",
    "\n",
    "  print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "  return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, criterion, val_dataloader_iter, validation_steps, device, \n",
    "             metric_agg_fn=None):\n",
    "  model.eval()  # Set model to evaluate mode\n",
    "\n",
    "  # statistics\n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0\n",
    "\n",
    "  # Iterate over all the validation data.\n",
    "  for step in range(validation_steps):\n",
    "    pd_batch = next(val_dataloader_iter)\n",
    "    inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device)\n",
    "\n",
    "    # Do not track history in evaluation to save memory\n",
    "    with torch.set_grad_enabled(False):\n",
    "      # forward\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item()\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "  \n",
    "  # The losses are averaged across observations for each minibatch.\n",
    "  epoch_loss = running_loss / validation_steps\n",
    "  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n",
    "  \n",
    "  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n",
    "  if metric_agg_fn is not None:\n",
    "    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n",
    "    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n",
    "\n",
    "  print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "  return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46242cb1-f02d-4940-9aae-a265ad4340da",
   "metadata": {},
   "source": [
    "## 3) Train and evaluate the model on the local machine\n",
    "Use converter.make_torch_dataloader(...) to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2d07be4-4413-4268-b201-2b3cd45d3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0927b0d-6066-4ef2-a207-d53afd7dc00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n",
      "Train Loss: 2.7889 Acc: 0.5425\n",
      "Validation Loss: 4.8211 Acc: 0.5354\n",
      "Epoch 2/2\n",
      "----------\n",
      "Train Loss: 2.8595 Acc: 0.5657\n",
      "Validation Loss: 5.4731 Acc: 0.5424\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = get_model(lr=lr)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Only parameters of final layer are being optimized.\n",
    "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                             batch_size=BATCH_SIZE) as train_dataloader, \\\n",
    "       converter_val.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                           batch_size=BATCH_SIZE) as val_dataloader:\n",
    "    \n",
    "        train_dataloader_iter = iter(train_dataloader)\n",
    "        steps_per_epoch = len(converter_train) // BATCH_SIZE\n",
    "        \n",
    "        val_dataloader_iter = iter(val_dataloader)\n",
    "        validation_steps = max(1, len(converter_val) // BATCH_SIZE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "          print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n",
    "          print('-' * 10)\n",
    "        \n",
    "          train_loss, train_acc = train_one_epoch(model, criterion, optimizer, exp_lr_scheduler, \n",
    "                                                  train_dataloader_iter, steps_per_epoch, epoch, \n",
    "                                                  device)\n",
    "          val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps, device)\n",
    "    return val_loss\n",
    "\n",
    "loss = train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ec9ad4-805e-48b7-989b-47df85d8393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab1be9-3106-4d5e-8b1c-f8712af89732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
