{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f533b06-99f6-443a-bc91-7cb9733a9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad44e98c-b2f1-4995-804f-1c7e8e59e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(start):\n",
    "    print(f'Elapsed time: {time.time() - start:.2f} s')\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc38141-ec20-4819-9f7b-ef2351bb7131",
   "metadata": {},
   "source": [
    "# Start Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcd3f2c-e8ee-4a74-baf8-6bc4c10b3f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/14 10:51:30 WARN Utils: Your hostname, bdai-desktop resolves to a loopback address: 127.0.1.1; using 165.132.118.199 instead (on interface enp0s31f6)\n",
      "23/08/14 10:51:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/14 10:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCPU').config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa6673-00b3-4ce6-80d1-4783fd5ba85a",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87460605-bc29-430d-b789-2d9d39c869b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = \"/home/bdai/spark_work/spark-warehouse/covid_train_binary\"\n",
    "test_image_path = \"/home/bdai/spark_work/spark-warehouse/covid_test_binary\"\n",
    "cache_path = \"file:///home/bdai/spark_work/petastorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e52230e-c6c5-4097-b238-92d8e2033ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df = spark.read.parquet(train_image_path, engine='fastparquet')\n",
    "df_test = spark.read.parquet(test_image_path, engine='fastparquet')\n",
    "\n",
    "df_train, df_val = train_df.randomSplit([0.8, 0.2], seed=12345)\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a00087-2e8e-4a7f-a9e9-a40a088b939d",
   "metadata": {},
   "source": [
    "# 2. Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed78f42-f3bd-43be-9660-d1a3d1eb3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from petastorm import TransformSpec\n",
    "\n",
    "image_shape = (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9fcc8-79ef-4c8b-a48e-991b3602331c",
   "metadata": {},
   "source": [
    "## 1) Cache the Spark DataFrame using Petastorm Spark converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f58c73-0a49-40fd-a37f-0795ca1061aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting floating-point columns to float32\n",
      "23/08/14 10:51:57 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 360/144\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 60 slabs, 114,587,997 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 114,587,997/114,587,997\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1611\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,611/1,731\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/128\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:3021\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 3,021/3,141\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:92\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/212\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:96\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/216\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/14 10:52:02 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 336/136\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 56 slabs, 108,896,667 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 108,896,667/108,896,667\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1379\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,379/1,491\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/120\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2695\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,695/2,807\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:60\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/172\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:60\n",
      "   data: initial: values:112\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112/172\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/14 10:52:02 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 360/144\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 60 slabs, 106,755,018 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 106,755,018/106,755,018\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1392\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,392/1,512\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/128\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2802\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,802/2,922\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/188\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:72\n",
      "   data: initial: values:120\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 120/192\n",
      " }\n",
      "}\n",
      "\n",
      "23/08/14 10:52:02 WARN InternalParquetRecordWriter: Too much memory used: Store {\n",
      " [class] optional binary class (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:24\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 384/152\n",
      " }\n",
      " [content] optional binary content {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:0\n",
      "   data: initial: values:0\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 64 slabs, 112,952,667 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 112,952,667/112,952,667\n",
      " }\n",
      " [file_name] optional binary file_name (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:1482\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 1,482/1,610\n",
      " }\n",
      " [label] optional float label {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:8\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/136\n",
      " }\n",
      " [path] optional binary path (STRING) {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:2986\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 2,986/3,114\n",
      " }\n",
      " [size, height] optional int32 height {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:68\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/196\n",
      " }\n",
      " [size, width] optional int32 width {\n",
      "  r:0 bytes\n",
      "  d:0 bytes\n",
      "   data: FallbackValuesWriter{\n",
      "   data: initial: DictionaryValuesWriter{\n",
      "   data: initial: dict:100\n",
      "   data: initial: values:128\n",
      "   data: initial:}\n",
      "\n",
      "   data: fallback: PLAIN CapacityByteArrayOutputStream 0 slabs, 0 bytes\n",
      "   data:}\n",
      "\n",
      "   pages: ColumnChunkPageWriter ConcatenatingByteArrayCollector 0 slabs, 0 bytes\n",
      "   total: 128/228\n",
      " }\n",
      "}\n",
      "\n",
      "Converting floating-point columns to float32                                    \n",
      "The median size 17372240 B (< 50 MB) of the parquet files is too small. Total size: 2608736962 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///home/bdai/spark_work/petastorm/20230814105228-appid-local-1691977891160-ce89d405-5fec-46e3-a243-4552a6ff50c2/part-00010-7cd6bc80-7bd1-4063-b5c8-62f79226ec36-c000.parquet, ...\n",
      "Converting floating-point columns to float32\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 23931, val: 6055, test : 400\n",
      "Elapsed time: 96.79 s\n"
     ]
    }
   ],
   "source": [
    "# Set a cache directory on DBFS FUSE for intermediate data.\n",
    "start = time.time()\n",
    "\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, cache_path)\n",
    "\n",
    "converter_train = make_spark_converter(df_train)\n",
    "converter_val = make_spark_converter(df_val)\n",
    "converter_test = make_spark_converter(df_test)\n",
    "\n",
    "print(f\"train: {len(converter_train)}, val: {len(converter_val)}, test : {len(converter_test)}\")\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09948a01-db59-46dd-93de-d14a5e22c932",
   "metadata": {},
   "source": [
    "## 2) Preprocess images\n",
    "Before feeding the dataset into the model, we need to decode the raw image bytes and apply standard ImageNet transforms. We recommend not doing this transformation on the Spark DataFrame since that will substantially increase the size of the intermediate files and might harm the performance. Instead, we recommend doing this transformation in a TransformSpec function in petastorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0995b149-a6b3-4581-b8b7-f1ed66b12666",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7118c5e-6552-4a6c-bb20-744f860d841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    image = Image.open(io.BytesIO(content)).resize([image_shape[1],image_shape[2]])\n",
    "    transformers = [transforms.Lambda(lambda image: image.convert('RGB'))]\n",
    "    transformers.extend([transforms.ToTensor()])\n",
    "    trans = transforms.Compose(transformers)\n",
    "    image_arr = trans(image)\n",
    "    return image_arr.numpy()\n",
    "    \n",
    "\n",
    "def transform_row(pd_batch):\n",
    "  \"\"\"\n",
    "  The input and output of this function must be pandas dataframes.\n",
    "  \"\"\"\n",
    "  start = time.time()\n",
    "  pd_batch['features'] = pd_batch['content'].map(lambda x: preprocess(x))\n",
    "  pd_batch['label'] = pd_batch['label'].map(lambda x: int(x))\n",
    "  pd_batch = pd_batch.drop(labels=['content'], axis=1)\n",
    "  end = time.time()\n",
    "  transform_time = end - start\n",
    "  time_list.append(transform_time)\n",
    "  return pd_batch[['features', 'label']]\n",
    "\n",
    "def get_transform_spec():\n",
    "  # Note that the output shape of the `TransformSpec` is not automatically known by petastorm, \n",
    "  # so we need to specify the shape for new columns in `edit_fields` and specify the order of \n",
    "  # the output columns in `selected_fields`.\n",
    "  return TransformSpec(transform_row, \n",
    "                       edit_fields=[('features', np.float32, image_shape, False)], \n",
    "                       selected_fields=['features', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487492df-2a5b-4934-8c15-bfd720e3d2bc",
   "metadata": {},
   "source": [
    "## 3) Examining execution time for dataloading and transorming a batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0685da20-4eb8-49e6-be2c-1b94f478fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/373 [00:00<?, ?it/s]Worker 2 terminated: unexpected exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py\", line 62, in run\n",
      "    self._worker_impl.process(*args, **kargs)\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 163, in process\n",
      "    all_cols = self._local_cache.get(cache_key,\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/cache.py\", line 39, in get\n",
      "    return fill_cache_func()\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 164, in <lambda>\n",
      "    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 196, in _load_rows\n",
      "    transformed_result = self._transform_spec.func(result_as_pandas)\n",
      "  File \"/tmp/ipykernel_3100/3118343967.py\", line 20, in transform_row\n",
      "    time_list.append(transform_time)\n",
      "NameError: name 'time_list' is not defined\n",
      "Worker 3 terminated: unexpected exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py\", line 62, in run\n",
      "    self._worker_impl.process(*args, **kargs)\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 163, in process\n",
      "    all_cols = self._local_cache.get(cache_key,\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/cache.py\", line 39, in get\n",
      "    return fill_cache_func()\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 164, in <lambda>\n",
      "    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 196, in _load_rows\n",
      "    transformed_result = self._transform_spec.func(result_as_pandas)\n",
      "  File \"/tmp/ipykernel_3100/3118343967.py\", line 20, in transform_row\n",
      "    time_list.append(transform_time)\n",
      "NameError: name 'time_list' is not defined\n",
      "Worker 0 terminated: unexpected exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py\", line 62, in run\n",
      "    self._worker_impl.process(*args, **kargs)\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 163, in process\n",
      "    all_cols = self._local_cache.get(cache_key,\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/cache.py\", line 39, in get\n",
      "    return fill_cache_func()\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 164, in <lambda>\n",
      "    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 196, in _load_rows\n",
      "    transformed_result = self._transform_spec.func(result_as_pandas)\n",
      "  File \"/tmp/ipykernel_3100/3118343967.py\", line 20, in transform_row\n",
      "    time_list.append(transform_time)\n",
      "NameError: name 'time_list' is not defined\n",
      "Worker 1 terminated: unexpected exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py\", line 62, in run\n",
      "    self._worker_impl.process(*args, **kargs)\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 163, in process\n",
      "    all_cols = self._local_cache.get(cache_key,\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/cache.py\", line 39, in get\n",
      "    return fill_cache_func()\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 164, in <lambda>\n",
      "    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\n",
      "  File \"/home/bdai/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py\", line 196, in _load_rows\n",
      "    transformed_result = self._transform_spec.func(result_as_pandas)\n",
      "  File \"/tmp/ipykernel_3100/3118343967.py\", line 20, in transform_row\n",
      "    time_list.append(transform_time)\n",
      "NameError: name 'time_list' is not defined\n",
      "Iteration on Petastorm DataLoader raise error: NameError(\"name 'time_list' is not defined\")\n",
      "  0%|                                                   | 0/373 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     train_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(steps_per_epoch)):\n\u001b[0;32m----> 7\u001b[0m         pd_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m pd_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), pd_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m timing(start)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/pytorch.py:121\u001b[0m, in \u001b[0;36mLoaderBase.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_impl():\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/pytorch.py:193\u001b[0m, in \u001b[0;36mDataLoader._iter_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuffling_buffer \u001b[38;5;241m=\u001b[39m NoopShufflingBuffer()\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Default collate does not work nicely on namedtuples and treat them as lists\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Using dict will result in the yielded structures being dicts as well\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     row_as_dict \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39m_asdict()\n\u001b[1;32m    198\u001b[0m     keys \u001b[38;5;241m=\u001b[39m row_as_dict\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/reader.py:695\u001b[0m, in \u001b[0;36mReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to read a sample after a reader created by \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    691\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_reader/make_batch_reader has stopped. This may happen if the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    692\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_reader/make_batch_reader context manager has exited but you try to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    693\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfetch a sample from it anyway\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_results_queue_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_row_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py:42\u001b[0m, in \u001b[0;36mArrowReaderWorkerResultsQueueReader.read_next\u001b[0;34m(self, workers_pool, schema, ngram)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ngram, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArrowReader does not support ngrams for now\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 42\u001b[0m     result_table \u001b[38;5;241m=\u001b[39m \u001b[43mworkers_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Convert arrow table columns into numpy. Strings are handled differently since to_pandas() returns\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# numpy array of dtype=object.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     result_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py:172\u001b[0m, in \u001b[0;36mThreadPool.get_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/workers_pool/thread_pool.py:62\u001b[0m, in \u001b[0;36mWorkerThread.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     (args, kargs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ventilator_queue\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39mIO_TIMEOUT_INTERVAL_S)\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_impl\u001b[38;5;241m.\u001b[39mpublish_func(VentilatedItemProcessedMessage())\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py:163\u001b[0m, in \u001b[0;36mArrowReaderWorker.process\u001b[0;34m(self, piece_index, worker_predicate, shuffle_row_drop_partition)\u001b[0m\n\u001b[1;32m    160\u001b[0m         path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_path_or_paths\n\u001b[1;32m    161\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hashlib\u001b[38;5;241m.\u001b[39mmd5(path_str\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest(),\n\u001b[1;32m    162\u001b[0m                                   piece\u001b[38;5;241m.\u001b[39mpath, piece_index)\n\u001b[0;32m--> 163\u001b[0m     all_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpiece\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_row_drop_partition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_cols:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpublish_func(all_cols)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/cache.py:39\u001b[0m, in \u001b[0;36mNullCache.get\u001b[0;34m(self, key, fill_cache_func)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, fill_cache_func):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfill_cache_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py:164\u001b[0m, in \u001b[0;36mArrowReaderWorker.process.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m         path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_path_or_paths\n\u001b[1;32m    161\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hashlib\u001b[38;5;241m.\u001b[39mmd5(path_str\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest(),\n\u001b[1;32m    162\u001b[0m                                   piece\u001b[38;5;241m.\u001b[39mpath, piece_index)\n\u001b[1;32m    163\u001b[0m     all_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_cache\u001b[38;5;241m.\u001b[39mget(cache_key,\n\u001b[0;32m--> 164\u001b[0m                                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpiece\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_row_drop_partition\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_cols:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpublish_func(all_cols)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark_env/lib/python3.9/site-packages/petastorm/arrow_reader_worker.py:196\u001b[0m, in \u001b[0;36mArrowReaderWorker._load_rows\u001b[0;34m(self, pq_file, piece, shuffle_row_drop_range)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# A user may omit `func` value if they intend just to delete some fields using the TransformSpec\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_spec\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[0;32m--> 196\u001b[0m     transformed_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_as_pandas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     transformed_result \u001b[38;5;241m=\u001b[39m result_as_pandas\n",
      "Cell \u001b[0;32mIn[55], line 20\u001b[0m, in \u001b[0;36mtransform_row\u001b[0;34m(pd_batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m transform_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtime_list\u001b[49m\u001b[38;5;241m.\u001b[39mappend(transform_time)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd_batch[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_list' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "steps_per_epoch = len(converter_train) // 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), batch_size=64) as train_dataloader:\n",
    "    train_dataloader_iter = iter(train_dataloader)\n",
    "    for step in tqdm(range(steps_per_epoch)):\n",
    "        pd_batch = next(train_dataloader_iter)\n",
    "        inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device)\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96aea774-4d01-4a73-b558-c948976aceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 373/373 [01:29<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "total_time = 0\n",
    "total_trans_time = 0\n",
    "time_start = time.time()\n",
    "start = time.time()\n",
    "steps_per_epoch = len(converter_train) // 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), batch_size=64) as train_dataloader:\n",
    "    train_dataloader_iter = iter(train_dataloader)\n",
    "    for step in tqdm(range(steps_per_epoch)):\n",
    "        pd_batch = next(train_dataloader_iter)\n",
    "        inputs, labels = pd_batch['features'], pd_batch['label']\n",
    "        time_end = time.time()\n",
    "        cur_time = time_end - time_start\n",
    "        total_time += cur_time\n",
    "        total_trans_time += trans_time.sum().item()\n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d066bb4-bb4b-472d-8ba6-d42306142ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "268"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b85923-a6ba-445a-aa32-1444e4bdeecb",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5d367-85ec-4fa0-8fff-709042c6b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a10f4-d7e2-485d-b837-31d42cc0b06c",
   "metadata": {},
   "source": [
    "## 1) Get the model ResNet from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31319536-7a2f-443c-b956-a97e82a220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(lr=0.001):\n",
    "  # Load a ResNet50 model from torchvision\n",
    "  model = torchvision.models.resnet50(pretrained=True)\n",
    "  # Freeze parameters in the feature extraction layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "  # Add a new classifier layer for transfer learning\n",
    "  num_ftrs = model.fc.in_features\n",
    "  # Parameters of newly constructed modules have requires_grad=True by default\n",
    "  model.fc = torch.nn.Sequential(torch.nn.Linear(num_ftrs, 1), torch.nn.Sigmoid())\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3592c-c981-47d0-9b96-d1c1f0e9bb5e",
   "metadata": {},
   "source": [
    "## 2) Define the train and evaluate function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7416cea7-290b-4d47-8deb-bb79167bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, scheduler, \n",
    "                    train_dataloader_iter, steps_per_epoch, epoch, \n",
    "                    device):\n",
    "    # statistics\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_loading = 0\n",
    "    total_training = 0 \n",
    "    total_stats = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    model.train()  # Set model to training mode\n",
    "    # Iterate over the data for one epoch.\n",
    "    load_start = time.time()\n",
    "    for step in tqdm(range(steps_per_epoch)):\n",
    "        pd_batch = next(train_dataloader_iter)\n",
    "        inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device).reshape(-1,1).float()\n",
    "        load_end = time.time()\n",
    "        current_load_time = load_end-load_start\n",
    "        total_loading += current_load_time\n",
    "        # Track history in training\n",
    "        train_start = time.time()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs > 0.5\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_end = time.time()\n",
    "        current_train_time = train_end-train_start\n",
    "        total_training += current_train_time\n",
    "        # stats_start = time.time()\n",
    "        # # statistics\n",
    "        # running_loss += loss.item()\n",
    "        # running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # stats_end = time.time()\n",
    "        # current_stats_time = stats_end - stats_start\n",
    "        # total_stats += current_stats_time\n",
    "        load_start = time.time()\n",
    "\n",
    "    scheduler.step()\n",
    "    end = time.time()\n",
    "    total_time = end-start\n",
    "    \n",
    "    # epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n",
    "    # epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n",
    "    print(\"Total time per epoch : {:.2f} / Dataloading time : {:.2f}  / Training time : {:.2f}\"\\\n",
    "          .format(total_time, total_loading, total_training))\n",
    "    # print(\"Total time per epoch : {:.2f} / Dataloading time : {:.2f}  / Training time : {:.2f} / Writing stats : {.2f}\"\\\n",
    "    #       .format(total_time, total_loading, total_training, total_stats))\n",
    "    # print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, criterion, val_dataloader_iter, validation_steps, device, \n",
    "             metric_agg_fn=None):\n",
    "  model.eval()  # Set model to evaluate mode\n",
    "\n",
    "  # statistics\n",
    "  running_loss = 0.0\n",
    "  running_corrects = 0\n",
    "\n",
    "  # Iterate over all the validation data.\n",
    "  for step in range(validation_steps):\n",
    "    pd_batch = next(val_dataloader_iter)\n",
    "    inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device).reshape(-1,1).float()\n",
    "\n",
    "    # Do not track history in evaluation to save memory\n",
    "    with torch.set_grad_enabled(False):\n",
    "      # forward\n",
    "      outputs = model(inputs)\n",
    "      preds = outputs > 0.5\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.item()\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "  \n",
    "  # The losses are averaged across observations for each minibatch.\n",
    "  epoch_loss = running_loss / validation_steps\n",
    "  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n",
    "  \n",
    "  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n",
    "  if metric_agg_fn is not None:\n",
    "    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n",
    "    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n",
    "\n",
    "  print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "  return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46242cb1-f02d-4940-9aae-a265ad4340da",
   "metadata": {},
   "source": [
    "## 3) Train and evaluate the model on the local machine\n",
    "Use converter.make_torch_dataloader(...) to create the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a2d07be4-4413-4268-b201-2b3cd45d3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# hyperparameters\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0927b0d-6066-4ef2-a207-d53afd7dc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = get_model(lr=lr)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    # Only parameters of final layer are being optimized.\n",
    "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                             batch_size=BATCH_SIZE) as train_dataloader, \\\n",
    "       converter_val.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                           batch_size=BATCH_SIZE) as val_dataloader:\n",
    "    \n",
    "        train_dataloader_iter = iter(train_dataloader)\n",
    "        steps_per_epoch = len(converter_train) // BATCH_SIZE\n",
    "        \n",
    "        val_dataloader_iter = iter(val_dataloader)\n",
    "        validation_steps = max(1, len(converter_val) // BATCH_SIZE)\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "          print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n",
    "          print('-' * 10)\n",
    "        \n",
    "          train_loss, train_acc = train_one_epoch(model, criterion, optimizer, exp_lr_scheduler, \n",
    "                                                  train_dataloader_iter, steps_per_epoch, epoch, \n",
    "                                                  device)\n",
    "          val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps, device)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab62a46-a777-487c-975d-21b54cc75e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 373/373 [02:06<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time per epoch : 126.05 / Dataloading time : 122.43  / Training time : 3.62\n",
      "Validation Loss: 3.1816 Acc: 0.5401\n",
      "Epoch 2/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████████████████▎ | 358/373 [01:58<00:04,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss = train_and_evaluate(1e-3)\n",
    "\n",
    "timing(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ec9ad4-805e-48b7-989b-47df85d8393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "14ba195e-eef8-49e4-a314-676331fc75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00047206878662109375 6.198883056640625e-05\n",
      "0.0005159378051757812 0.00021338462829589844\n",
      "0.0005242824554443359 0.00026106834411621094\n",
      "0.0005552768707275391 0.00032067298889160156\n",
      "0.0005648136138916016 0.0004048347473144531\n",
      "0.0005803108215332031 0.0004627704620361328\n",
      "0.0005886554718017578 0.0005068778991699219\n",
      "0.0005970001220703125 0.0005524158477783203\n",
      "0.0006055831909179688 0.0005993843078613281\n",
      "0.0006163120269775391 0.0006570816040039062\n",
      "0.0006499290466308594 0.0008254051208496094\n",
      "0.0006580352783203125 0.0008697509765625\n",
      "0.0006663799285888672 0.0009145736694335938\n",
      "0.0006756782531738281 0.0009696483612060547\n",
      "0.0006933212280273438 0.0010309219360351562\n",
      "0.0007028579711914062 0.0011048316955566406\n",
      "0.0007107257843017578 0.0011475086212158203\n",
      "0.0007188320159912109 0.0011904239654541016\n",
      "0.0007381439208984375 0.0012364387512207031\n",
      "0.0007460117340087891 0.001279592514038086\n",
      "0.0007541179656982422 0.0013232231140136719\n",
      "0.0007624626159667969 0.0013668537139892578\n",
      "0.0007710456848144531 0.0014109611511230469\n",
      "0.0007796287536621094 0.0014545917510986328\n",
      "0.0007939338684082031 0.0017154216766357422\n",
      "0.0008022785186767578 0.001760721206665039\n",
      "0.0008111000061035156 0.0018084049224853516\n",
      "0.0008292198181152344 0.0018987655639648438\n",
      "0.0008447170257568359 0.0020842552185058594\n",
      "0.0008525848388671875 0.00212860107421875\n",
      "0.0008606910705566406 0.0021719932556152344\n",
      "0.0008692741394042969 0.0022363662719726562\n",
      "0.0008814334869384766 0.002281665802001953\n",
      "0.0008893013000488281 0.0023267269134521484\n",
      "0.0008976459503173828 0.002371072769165039\n",
      "0.0009055137634277344 0.0024175643920898438\n",
      "0.0009279251098632812 0.0024635791778564453\n",
      "0.0009357929229736328 0.00251007080078125\n",
      "0.0009441375732421875 0.002555370330810547\n",
      "0.0009579658508300781 0.0026051998138427734\n",
      "0.0009655952453613281 0.002668142318725586\n",
      "0.0009732246398925781 0.0027151107788085938\n",
      "0.0009810924530029297 0.0027616024017333984\n",
      "0.0009906291961669922 0.0028069019317626953\n",
      "0.0009992122650146484 0.002850770950317383\n",
      "0.0010077953338623047 0.0028977394104003906\n",
      "0.0010156631469726562 0.0029418468475341797\n",
      "0.0010235309600830078 0.002986431121826172\n",
      "0.0010330677032470703 0.0030341148376464844\n",
      "0.0010416507720947266 0.0030813217163085938\n",
      "0.001050710678100586 0.003127574920654297\n",
      "0.001058816909790039 0.003179788589477539\n",
      "0.0010671615600585938 0.0032253265380859375\n",
      "0.0010762214660644531 0.003271818161010742\n",
      "0.001085042953491211 0.003323793411254883\n",
      "0.0010936260223388672 0.0033719539642333984\n",
      "0.0011038780212402344 0.003418445587158203\n",
      "0.0011134147644042969 0.003470897674560547\n",
      "0.0011577606201171875 0.003520488739013672\n",
      "0.0011670589447021484 0.0035681724548339844\n",
      "0.0011768341064453125 0.003620147705078125\n",
      "0.0011844635009765625 0.0036649703979492188\n",
      "0.0011925697326660156 0.0037109851837158203\n",
      "0.0012013912200927734 0.0037581920623779297\n",
      "0.001209259033203125 0.0038270950317382812\n",
      "0.0012204647064208984 0.003878355026245117\n",
      "0.0012292861938476562 0.003929853439331055\n",
      "0.001238107681274414 0.003987789154052734\n",
      "0.0012481212615966797 0.004103183746337891\n",
      "0.0012607574462890625 0.004246234893798828\n",
      "0.0012736320495605469 0.004324197769165039\n",
      "0.001283407211303711 0.004388332366943359\n",
      "0.0012938976287841797 0.004450559616088867\n",
      "0.0013039112091064453 0.00450444221496582\n",
      "0.0013117790222167969 0.004552125930786133\n",
      "0.001322031021118164 0.004606485366821289\n",
      "0.0013301372528076172 0.0046520233154296875\n",
      "0.0013515949249267578 0.004700899124145508\n",
      "0.0013606548309326172 0.004751443862915039\n",
      "0.0013687610626220703 0.004796266555786133\n",
      "0.0013778209686279297 0.004842519760131836\n",
      "0.0013866424560546875 0.004886627197265625\n",
      "0.0013947486877441406 0.004929780960083008\n",
      "0.0014030933380126953 0.004974365234375\n",
      "0.00141143798828125 0.005022764205932617\n",
      "0.0014190673828125 0.005066394805908203\n",
      "0.0014281272888183594 0.005128145217895508\n",
      "0.0014710426330566406 0.005246877670288086\n",
      "0.0014793872833251953 0.0053234100341796875\n",
      "0.0014905929565429688 0.005391359329223633\n",
      "0.0015006065368652344 0.005453348159790039\n",
      "0.001508951187133789 0.0054967403411865234\n",
      "0.0015175342559814453 0.0055429935455322266\n",
      "0.0015254020690917969 0.0055887699127197266\n",
      "0.0015330314636230469 0.00563359260559082\n",
      "0.0015411376953125 0.0056781768798828125\n",
      "0.0015518665313720703 0.005818843841552734\n",
      "0.0015654563903808594 0.0058634281158447266\n",
      "0.0015895366668701172 0.005908489227294922\n",
      "0.0015976428985595703 0.005953788757324219\n",
      "0.0016160011291503906 0.006000518798828125\n",
      "0.0016236305236816406 0.0060443878173828125\n",
      "0.0016317367553710938 0.0060901641845703125\n",
      "0.0016422271728515625 0.006146669387817383\n",
      "0.0016505718231201172 0.0061948299407958984\n",
      "0.0016591548919677734 0.006242513656616211\n",
      "0.0016674995422363281 0.006287574768066406\n",
      "0.0016756057739257812 0.006339311599731445\n",
      "0.00168609619140625 0.0063855648040771484\n",
      "0.0017001628875732422 0.006488323211669922\n",
      "0.0017101764678955078 0.006546974182128906\n",
      "0.0018358230590820312 0.0065975189208984375\n",
      "0.0018439292907714844 0.0066432952880859375\n",
      "0.0018529891967773438 0.0066907405853271484\n",
      "0.0019068717956542969 0.0068759918212890625\n",
      "0.0019154548645019531 0.006926536560058594\n",
      "0.0019328594207763672 0.006978511810302734\n",
      "0.0019412040710449219 0.0070362091064453125\n",
      "0.0019488334655761719 0.007081270217895508\n",
      "0.001957416534423828 0.007128715515136719\n",
      "0.0019655227661132812 0.007174253463745117\n",
      "0.0019736289978027344 0.007220029830932617\n",
      "0.001987934112548828 0.007281303405761719\n",
      "0.0019960403442382812 0.007327079772949219\n",
      "0.0020318031311035156 0.007439136505126953\n",
      "0.0020415782928466797 0.007494449615478516\n",
      "0.00205230712890625 0.007561206817626953\n",
      "0.0020661354064941406 0.007651090621948242\n",
      "0.0020744800567626953 0.007696390151977539\n",
      "0.0020821094512939453 0.007740497589111328\n",
      "0.0020933151245117188 0.00779271125793457\n",
      "0.002102375030517578 0.007849454879760742\n",
      "0.0021109580993652344 0.00789332389831543\n",
      "0.002128124237060547 0.00795125961303711\n",
      "0.002166271209716797 0.008008956909179688\n",
      "0.0021791458129882812 0.008064746856689453\n",
      "0.0021910667419433594 0.008122682571411133\n",
      "0.002198934555053711 0.008167266845703125\n",
      "0.002207040786743164 0.008213043212890625\n",
      "0.0022156238555908203 0.008259057998657227\n",
      "0.002223968505859375 0.008303403854370117\n",
      "0.0022318363189697266 0.00834798812866211\n",
      "0.002247333526611328 0.008396625518798828\n",
      "0.0022547245025634766 0.00846242904663086\n",
      "0.0022630691528320312 0.008552312850952148\n",
      "0.0022721290588378906 0.008605003356933594\n",
      "0.00228118896484375 0.008652448654174805\n",
      "0.0022919178009033203 0.008713006973266602\n",
      "0.0023000240325927734 0.008759021759033203\n",
      "0.002309560775756836 0.008804082870483398\n",
      "0.002318859100341797 0.008847951889038086\n",
      "0.002329587936401367 0.008897781372070312\n",
      "0.002337932586669922 0.008948087692260742\n",
      "0.002346038818359375 0.008994340896606445\n",
      "0.002357006072998047 0.009057044982910156\n",
      "0.0023648738861083984 0.009104251861572266\n",
      "0.00237274169921875 0.009149551391601562\n",
      "0.002380847930908203 0.009196281433105469\n",
      "0.0023889541625976562 0.009239673614501953\n",
      "0.0024149417877197266 0.009319305419921875\n",
      "0.002422809600830078 0.009365320205688477\n",
      "0.0024309158325195312 0.009409189224243164\n",
      "0.002439260482788086 0.00945425033569336\n",
      "0.002446889877319336 0.009505510330200195\n",
      "0.0024547576904296875 0.00955057144165039\n",
      "0.002475738525390625 0.009608268737792969\n",
      "0.0024857521057128906 0.009663820266723633\n",
      "0.0024940967559814453 0.009709358215332031\n",
      "0.002501964569091797 0.009754657745361328\n",
      "0.0025098323822021484 0.009799480438232422\n",
      "0.002530813217163086 0.009847879409790039\n",
      "0.0025501251220703125 0.009892702102661133\n",
      "0.002557039260864258 0.009936094284057617\n",
      "0.0025649070739746094 0.009979009628295898\n",
      "0.002572298049926758 0.010023117065429688\n",
      "0.002579927444458008 0.010067462921142578\n",
      "0.002588510513305664 0.010111808776855469\n",
      "0.002596139907836914 0.010156631469726562\n",
      "0.0026063919067382812 0.010242700576782227\n",
      "0.0026154518127441406 0.010377168655395508\n",
      "0.0026235580444335938 0.010423660278320312\n",
      "0.002633333206176758 0.010476827621459961\n",
      "0.0026445388793945312 0.010551691055297852\n",
      "0.0026731491088867188 0.01061868667602539\n",
      "0.002681732177734375 0.010665655136108398\n",
      "0.002689361572265625 0.010709762573242188\n",
      "0.002697467803955078 0.010758399963378906\n",
      "0.002705097198486328 0.010804414749145508\n",
      "0.0027163028717041016 0.010866403579711914\n",
      "0.002724885940551758 0.010913610458374023\n",
      "0.002732515335083008 0.010956525802612305\n",
      "0.002740621566772461 0.010999202728271484\n",
      "0.0027489662170410156 0.011044979095458984\n",
      "0.0027570724487304688 0.011087894439697266\n",
      "0.002773761749267578 0.01113271713256836\n",
      "0.0027811527252197266 0.011176109313964844\n",
      "0.002789020538330078 0.011227607727050781\n",
      "0.002796649932861328 0.011271238327026367\n",
      "0.002803802490234375 0.011343002319335938\n",
      "0.002819061279296875 0.011388301849365234\n",
      "0.0028269290924072266 0.011452674865722656\n",
      "0.002834796905517578 0.011497259140014648\n",
      "0.002843141555786133 0.011544942855834961\n",
      "0.002851247787475586 0.011595487594604492\n",
      "0.0028612613677978516 0.011657476425170898\n",
      "0.0028700828552246094 0.01170659065246582\n",
      "0.0028781890869140625 0.011750459671020508\n",
      "0.002886056900024414 0.011800289154052734\n",
      "0.002896547317504883 0.011854410171508789\n",
      "0.002904176712036133 0.011898279190063477\n",
      "0.0029115676879882812 0.011941909790039062\n",
      "0.0029191970825195312 0.011986732482910156\n",
      "0.002927064895629883 0.012032270431518555\n",
      "0.002934694290161133 0.012076854705810547\n",
      "0.002943277359008789 0.012140750885009766\n",
      "0.00295257568359375 0.012186050415039062\n",
      "0.0029692649841308594 0.012272834777832031\n",
      "0.0029768943786621094 0.012320756912231445\n",
      "0.0029850006103515625 0.012365341186523438\n",
      "0.0029942989349365234 0.012427330017089844\n",
      "0.0030028820037841797 0.012471914291381836\n",
      "0.0030112266540527344 0.012517213821411133\n",
      "0.0030198097229003906 0.012566089630126953\n",
      "0.003027677536010742 0.01261138916015625\n",
      "0.003036975860595703 0.012656211853027344\n",
      "0.0030481815338134766 0.01270747184753418\n",
      "0.003130674362182617 0.012766361236572266\n",
      "0.0031385421752929688 0.012818098068237305\n",
      "0.0031576156616210938 0.012871265411376953\n",
      "0.0031681060791015625 0.012940645217895508\n",
      "0.003177642822265625 0.01298666000366211\n",
      "0.0031855106353759766 0.013033628463745117\n",
      "0.003214597702026367 0.013199329376220703\n",
      "0.0032248497009277344 0.013264894485473633\n",
      "0.0032341480255126953 0.01331019401550293\n",
      "0.003242015838623047 0.013355016708374023\n",
      "0.003251791000366211 0.013402223587036133\n",
      "0.0032596588134765625 0.01345062255859375\n",
      "0.003267526626586914 0.013506174087524414\n",
      "0.0032782554626464844 0.013552188873291016\n",
      "0.003290414810180664 0.013599634170532227\n",
      "0.003300189971923828 0.013661384582519531\n",
      "0.0033082962036132812 0.01371312141418457\n",
      "0.003317117691040039 0.013770103454589844\n",
      "0.0033271312713623047 0.013834238052368164\n",
      "0.0033388137817382812 0.013909339904785156\n",
      "0.003347635269165039 0.013954877853393555\n",
      "0.0033659934997558594 0.014028549194335938\n",
      "0.0033817291259765625 0.014125823974609375\n",
      "0.003431081771850586 0.014253377914428711\n",
      "0.003441333770751953 0.014317512512207031\n",
      "0.003449678421020508 0.014371395111083984\n",
      "0.0034580230712890625 0.014419317245483398\n",
      "0.003465890884399414 0.014464616775512695\n",
      "0.0034918785095214844 0.014532804489135742\n",
      "0.0035047531127929688 0.01459193229675293\n",
      "0.003513336181640625 0.014637231826782227\n",
      "0.003522157669067383 0.014682292938232422\n",
      "0.0035305023193359375 0.014733314514160156\n",
      "0.0035505294799804688 0.014927148818969727\n",
      "0.0035707950592041016 0.01497793197631836\n",
      "0.0035810470581054688 0.015034198760986328\n",
      "0.003594636917114258 0.015111446380615234\n",
      "0.003602743148803711 0.015155792236328125\n",
      "0.0036187171936035156 0.015227794647216797\n",
      "0.003627300262451172 0.015273332595825195\n",
      "0.003636598587036133 0.015324115753173828\n",
      "0.0036458969116210938 0.015369653701782227\n",
      "0.0036537647247314453 0.01541590690612793\n",
      "0.003662586212158203 0.015466690063476562\n",
      "0.003670215606689453 0.015510320663452148\n",
      "0.0036780834197998047 0.015555143356323242\n",
      "0.0036857128143310547 0.015598535537719727\n",
      "0.003693819046020508 0.015644311904907227\n",
      "0.0037026405334472656 0.01569819450378418\n",
      "0.003711223602294922 0.015746355056762695\n",
      "0.0037190914154052734 0.015791893005371094\n",
      "0.0037267208099365234 0.015837430953979492\n",
      "0.003735780715942383 0.01588273048400879\n",
      "0.00374603271484375 0.01593184471130371\n",
      "0.0037567615509033203 0.015979528427124023\n",
      "0.003765106201171875 0.016025781631469727\n",
      "0.0037746429443359375 0.01607203483581543\n",
      "0.003782987594604492 0.016117095947265625\n",
      "0.003790616989135742 0.016169071197509766\n",
      "0.0038001537322998047 0.01623821258544922\n",
      "0.0038084983825683594 0.016284465789794922\n",
      "0.003833293914794922 0.016346216201782227\n",
      "0.0038416385650634766 0.01639556884765625\n",
      "0.003858327865600586 0.016442537307739258\n",
      "0.0038661956787109375 0.01648712158203125\n",
      "0.003874063491821289 0.016530752182006836\n",
      "0.0038826465606689453 0.016575336456298828\n",
      "0.0038928985595703125 0.016631126403808594\n",
      "0.00390625 0.01670241355895996\n",
      "0.0039215087890625 0.016750097274780273\n",
      "0.003934383392333984 0.01679849624633789\n",
      "0.0039424896240234375 0.016845703125\n",
      "0.003950595855712891 0.016915321350097656\n",
      "0.003960371017456055 0.01697707176208496\n",
      "0.004109621047973633 0.017041444778442383\n",
      "0.004117727279663086 0.01708817481994629\n",
      "0.004127025604248047 0.017155170440673828\n",
      "0.004144906997680664 0.017215490341186523\n",
      "0.004153013229370117 0.01725912094116211\n",
      "0.004161834716796875 0.017320871353149414\n",
      "0.004179239273071289 0.017390966415405273\n",
      "0.004187583923339844 0.01743626594543457\n",
      "0.004195451736450195 0.01747918128967285\n",
      "0.004203081130981445 0.017525672912597656\n",
      "0.004210472106933594 0.017569541931152344\n",
      "0.00421905517578125 0.017615318298339844\n",
      "0.004227638244628906 0.01765894889831543\n",
      "0.004235267639160156 0.017702341079711914\n",
      "0.004243373870849609 0.01775050163269043\n",
      "0.004269838333129883 0.017829179763793945\n",
      "0.0042803287506103516 0.017899036407470703\n",
      "0.00429224967956543 0.01797008514404297\n",
      "0.004309892654418945 0.018261194229125977\n",
      "0.004317760467529297 0.018305063247680664\n",
      "0.0043256282806396484 0.018354415893554688\n",
      "0.004333972930908203 0.01839923858642578\n",
      "0.004341840744018555 0.018442630767822266\n",
      "0.0043506622314453125 0.018489360809326172\n",
      "0.0043604373931884766 0.018555641174316406\n",
      "0.004369258880615234 0.01860332489013672\n",
      "0.004377841949462891 0.018648624420166016\n",
      "0.004387378692626953 0.01869678497314453\n",
      "0.004395484924316406 0.01874232292175293\n",
      "0.004405498504638672 0.018792390823364258\n",
      "0.004413604736328125 0.018836259841918945\n",
      "0.0044214725494384766 0.018879175186157227\n",
      "0.004429340362548828 0.018928050994873047\n",
      "0.00443720817565918 0.01897287368774414\n",
      "0.00444483757019043 0.019026517868041992\n",
      "0.004452228546142578 0.019071102142333984\n",
      "0.00446009635925293 0.01911616325378418\n",
      "0.004467964172363281 0.019166946411132812\n",
      "0.004475593566894531 0.019212007522583008\n",
      "0.00448298454284668 0.0192563533782959\n",
      "0.004490852355957031 0.019306421279907227\n",
      "0.004498720169067383 0.019350290298461914\n",
      "0.004517078399658203 0.019394874572753906\n",
      "0.00452423095703125 0.019438505172729492\n",
      "0.0045318603515625 0.01957416534423828\n",
      "0.004539966583251953 0.01961827278137207\n",
      "0.004548072814941406 0.019664764404296875\n",
      "0.004556179046630859 0.019806623458862305\n",
      "0.004568338394165039 0.019851207733154297\n",
      "0.0045833587646484375 0.0198976993560791\n",
      "0.004591226577758789 0.019943952560424805\n",
      "0.004602193832397461 0.02000570297241211\n",
      "0.0046155452728271484 0.020073413848876953\n",
      "0.0046253204345703125 0.020119667053222656\n",
      "0.004637002944946289 0.02019047737121582\n",
      "0.004647254943847656 0.020247459411621094\n",
      "0.004656314849853516 0.0202939510345459\n",
      "0.00466465950012207 0.02033829689025879\n",
      "0.004674434661865234 0.020382165908813477\n",
      "0.004682779312133789 0.020427227020263672\n",
      "0.004690885543823242 0.020473718643188477\n",
      "0.004698514938354492 0.020523786544799805\n",
      "0.0047070980072021484 0.020569801330566406\n",
      "0.0047152042388916016 0.0206143856048584\n",
      "0.004723310470581055 0.0206601619720459\n",
      "0.004731416702270508 0.020705223083496094\n",
      "0.004748106002807617 0.020754098892211914\n",
      "0.0047566890716552734 0.020819664001464844\n",
      "0.004764080047607422 0.020864248275756836\n",
      "0.0047724246978759766 0.020914793014526367\n",
      "0.004788875579833984 0.020959854125976562\n",
      "0.004796743392944336 0.021004915237426758\n",
      "0.004808902740478516 0.021051883697509766\n",
      "Train Loss: 0.0241 Acc: 0.5743\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 1e-3\n",
    "model = get_model(lr=lr)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Only parameters of final layer are being optimized.\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "# statistics\n",
    "running_loss = torch.zeros(1).to(device)\n",
    "running_corrects = 0\n",
    "total_fir = 0\n",
    "total_sec = 0\n",
    "with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                         batch_size=BATCH_SIZE) as train_dataloader, \\\n",
    "    converter_val.make_torch_dataloader(transform_spec=get_transform_spec(), \n",
    "                                       batch_size=BATCH_SIZE) as val_dataloader:\n",
    "\n",
    "    train_dataloader_iter = iter(train_dataloader)\n",
    "    steps_per_epoch = len(converter_train) // BATCH_SIZE\n",
    "    \n",
    "    val_dataloader_iter = iter(val_dataloader)\n",
    "    validation_steps = max(1, len(converter_val) // BATCH_SIZE)\n",
    "\n",
    "    # Iterate over the data for one epoch.\n",
    "    for step in range(steps_per_epoch):\n",
    "        pd_batch = next(train_dataloader_iter)\n",
    "        inputs, labels = pd_batch['features'].to(device), pd_batch['label'].to(device).reshape(-1,1).float()\n",
    "        \n",
    "    \n",
    "        # Track history in training\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs > 0.5\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        fir_start = time.time()\n",
    "        running_loss += loss\n",
    "        fir_end = time.time()\n",
    "        fir = fir_end - fir_start\n",
    "        sec_start = time.time()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        sec_end = time.time()\n",
    "        sec = sec_end - sec_start\n",
    "        total_fir += fir\n",
    "        total_sec += sec\n",
    "        print(total_fir, total_sec)\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_loss = running_loss.item() / (steps_per_epoch * BATCH_SIZE)\n",
    "    epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n",
    "    \n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "835ff8b2-15e9-4d96-955f-af978e589dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4415], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.zeros(1).to(device)\n",
    "temp + loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
